# =============================================================================
# Qwen Fine Tune Test Suite - Docker Compose
# =============================================================================
#
# Full Stack Usage:
#   docker compose up                     # Start all services (frontend + backend)
#   docker compose up frontend backend    # Start web UI only
#   docker compose up -d                  # Start in background
#
# Legacy Services (training/evaluation):
#   docker compose run train              # Interactive training
#   docker compose run evaluate           # Model evaluation
#   docker compose run convert            # Convert to GGUF
#   docker compose up streamlit           # Legacy Streamlit UI (localhost:8501)
#
# =============================================================================

services:
  # ===========================
  # Full Stack Web Application
  # ===========================

  # Backend API (FastAPI)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: qwen-lab-backend
    container_name: qwen-backend
    volumes:
      - ./outputs:/app/outputs
      - ./models:/app/models
      - ./evaluation_results:/app/evaluation_results
      - ./configs:/app/configs
      - ./train.py:/app/train.py
      - ./evaluate_model.py:/app/evaluate_model.py
      - ./merge_and_convert_gguff.py:/app/merge_and_convert_gguff.py
      - ./qwen_suite.db:/app/qwen_suite.db
      - huggingface_cache:/root/.cache/huggingface
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # Frontend (React + Vite)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: qwen-lab-frontend
    container_name: qwen-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped

  # ===========================
  # Legacy Services
  # ===========================

  # Base service with common settings
  base:
    build: .
    image: qwen-lab
    volumes:
      - ./outputs:/app/outputs
      - ./models:/app/models
      - ./evaluation_results:/app/evaluation_results
      - ./configs:/app/configs
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Training service
  train:
    extends: base
    stdin_open: true
    tty: true
    entrypoint: ["python", "train.py"]

  # Evaluation service
  evaluate:
    extends: base
    stdin_open: true
    tty: true
    entrypoint: ["python", "evaluate_model.py"]

  # GGUF conversion service
  convert:
    extends: base
    stdin_open: true
    tty: true
    entrypoint: ["python", "merge_and_convert_gguff.py"]

  # Legacy Streamlit app
  streamlit:
    extends: base
    ports:
      - "8501:8501"
    entrypoint: ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]

# Named volume for HuggingFace cache (persists across runs)
volumes:
  huggingface_cache:
