# Default Training Configuration (Production)
# Based on best practices from Qwen32b_chem_dataset_tune.py
name: "default"
description: "Production training settings with evaluation and checkpointing"

# Batch size and gradient accumulation
per_device_train_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16

# Training duration
num_train_epochs: 1
max_steps: -1  # -1 means use num_train_epochs

# Learning rate and scheduler
learning_rate: 5.0e-5
lr_scheduler_type: "cosine"
warmup_steps: 10
max_grad_norm: 1.0
weight_decay: 0.01

# Optimizer
optim: "adamw_8bit"

# Precision (auto-detect bfloat16 support)
auto_precision: true  # Will use bf16 if supported, else fp16

# Logging
logging_steps: 1

# Checkpointing
save_strategy: "steps"
save_steps: 25
save_total_limit: 3  # Keep only last 3 checkpoints

# Evaluation
eval_strategy: "steps"
eval_steps: 25

# Output
output_dir: "outputs"
push_to_hub: false

# Reproducibility
seed: 3407

# WandB project naming template
# Available placeholders: {model_name}, {dataset_name}, {model_size}
wandb_project_template: "{dataset_name}-{model_name}"
